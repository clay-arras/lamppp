<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>lamppp: Understanding Autograd</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">lamppp
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="index.html">Lamp++ Documentation</a></li><li class="navelem"><a class="el" href="using_tensor.html">Working with Tensors</a></li>  </ul>
</div>
</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">Understanding Autograd</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Automatic differentiation is the heart of modern machine learning. Lamp++'s autograd system builds computation graphs on-the-fly and computes gradients using the chain rule. This guide explains how it works and how to use it effectively.</p>
<h2><a class="anchor" id="autotoc_md20"></a>
The big picture</h2>
<p>When you do math with Variables (instead of plain Tensors), Lamp++ secretly builds a computation graph. Each operation creates a node that remembers how to compute gradients. When you call <code>.backward()</code>, it walks through this graph backward, computing derivatives using the chain rule.</p>
<p>Here's the simplest possible example:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;lamppp/lamppp.hpp&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">int</span> main() {</div>
<div class="line">    <span class="comment">// Create a variable that tracks gradients</span></div>
<div class="line">    <a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> data({2.0f}, {1}, lmp::DeviceType::CPU, lmp::DataType::Float32);</div>
<div class="line">    <a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> x(data, <span class="keyword">true</span>);  <span class="comment">// requires_grad = true</span></div>
<div class="line">    </div>
<div class="line">    <span class="comment">// Do some computation</span></div>
<div class="line">    <a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> y = x * x;      <span class="comment">// y = xÂ²</span></div>
<div class="line">    </div>
<div class="line">    <span class="comment">// Compute gradients</span></div>
<div class="line">    y.backward();</div>
<div class="line">    </div>
<div class="line">    <span class="comment">// dy/dx = 2x = 2 * 2.0 = 4.0</span></div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Gradient: &quot;</span> &lt;&lt; x.grad() &lt;&lt; std::endl;  <span class="comment">// Should print 4.0</span></div>
<div class="line">    </div>
<div class="line">    <span class="keywordflow">return</span> 0;</div>
<div class="line">}</div>
<div class="ttc" id="aclasslmp_1_1autograd_1_1Variable_html"><div class="ttname"><a href="classlmp_1_1autograd_1_1Variable.html">lmp::autograd::Variable</a></div><div class="ttdef"><b>Definition</b> variable.hpp:48</div></div>
<div class="ttc" id="aclasslmp_1_1tensor_1_1Tensor_html"><div class="ttname"><a href="classlmp_1_1tensor_1_1Tensor.html">lmp::tensor::Tensor</a></div><div class="ttdoc">Main tensor object for Lamppp.</div><div class="ttdef"><b>Definition</b> tensor.hpp:29</div></div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md21"></a>
Variables: Tensors with gradient tracking</h2>
<p>Variables are wrappers around Tensors that can track gradients. Every Variable has four key components:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> var(tensor_data, <span class="keyword">true</span>);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// The four components:</span></div>
<div class="line">var.data();         <span class="comment">// The actual tensor data</span></div>
<div class="line">var.grad();         <span class="comment">// Accumulated gradients (initially zero)</span></div>
<div class="line">var.requires_grad(); <span class="comment">// Whether to track gradients</span></div>
<div class="line">var.grad_fn();      <span class="comment">// Pointer to the operation that created this variable</span></div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md22"></a>
Creating Variables</h3>
<div class="fragment"><div class="line"><span class="comment">// From existing tensors</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> tensor(data, {2, 3}, lmp::DeviceType::CPU);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> var1(tensor, <span class="keyword">true</span>);  <span class="comment">// requires_grad = true</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Using autograd constructors</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> zeros_var = lmp::autograd::zeros({2, 3}, lmp::DeviceType::CPU, </div>
<div class="line">                                               lmp::DataType::Float32, <span class="keyword">true</span>);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> ones_var = lmp::autograd::ones({2, 3}, lmp::DeviceType::CPU, </div>
<div class="line">                                             lmp::DataType::Float32, <span class="keyword">true</span>);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> rand_var = lmp::autograd::rand({2, 3}, lmp::DeviceType::CPU, </div>
<div class="line">                                             lmp::DataType::Float32, <span class="keyword">true</span>);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// From nested vectors (automatically infers shape)</span></div>
<div class="line">std::vector&lt;std::vector&lt;float&gt;&gt; nested_data = {{1.0f, 2.0f}, {3.0f, 4.0f}};</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> tensor_var = lmp::autograd::tensor(nested_data, lmp::DeviceType::CPU,</div>
<div class="line">                                                 lmp::DataType::Float32, <span class="keyword">true</span>);</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md23"></a>
Gradient requirements</h3>
<p>Only Variables with <code>requires_grad=true</code> participate in gradient computation:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> a(tensor_a, <span class="keyword">true</span>);   <span class="comment">// Will track gradients</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> b(tensor_b, <span class="keyword">false</span>);  <span class="comment">// Won&#39;t track gradients</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> c = a + b;           <span class="comment">// Result tracks gradients (inherits from a)</span></div>
</div><!-- fragment --><p>Rule**: If any input requires gradients, the result requires gradients.</p>
<h2><a class="anchor" id="autotoc_md24"></a>
Operations and the computation graph</h2>
<p>Every operation on Variables creates a node in the computation graph:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> a(tensor_a, <span class="keyword">true</span>);   <span class="comment">// Leaf node</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> b(tensor_b, <span class="keyword">true</span>);   <span class="comment">// Leaf node</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> c = a * b;           <span class="comment">// Multiplication node</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> d = lmp::sum(c, 0);  <span class="comment">// Sum node</span></div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md25"></a>
What creates gradient nodes</h3>
<p>Operations that create backward nodes:**</p><ul>
<li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code></li>
<li>Math functions: <code>exp()</code>, <code>log()</code>, <code>sqrt()</code>, <code>sin()</code>, <code>cos()</code>, etc.</li>
<li>Reductions: <code>sum()</code>, <code>max()</code>, <code>min()</code>, <code>prod()</code></li>
<li>Matrix ops: <code>matmul()</code>, <code>transpose()</code></li>
<li><p class="startli">Shape ops: <code>reshape()</code>, <code>squeeze()</code>, <code>expand_dims()</code>, <code>to()</code> (device transfer)</p>
<p class="startli">Operations that DON'T create gradient nodes:**</p>
</li>
<li>Comparison ops: <code>==</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code>, etc. (return boolean tensors without gradients)</li>
<li>Data access: <code>.data()</code>, <code>.shape()</code>, <code>.device()</code>, etc.</li>
</ul>
<h2><a class="anchor" id="autotoc_md26"></a>
The backward pass</h2>
<p>The backward pass computes gradients using reverse-mode automatic differentiation:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> x(data, <span class="keyword">true</span>);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> y = x * x * x;  <span class="comment">// y = xÂ³</span></div>
<div class="line"> </div>
<div class="line">y.backward();  <span class="comment">// Compute dy/dx = 3xÂ²</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Access gradients</span></div>
<div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;dy/dx: &quot;</span> &lt;&lt; x.grad() &lt;&lt; std::endl;</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md27"></a>
Gradient accumulation</h3>
<p>Gradients <b>accumulate</b> by default:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> x(data, <span class="keyword">true</span>);</div>
<div class="line"> </div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> y1 = x * 2;</div>
<div class="line">y1.backward();</div>
<div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;First gradient: &quot;</span> &lt;&lt; x.grad() &lt;&lt; std::endl;  <span class="comment">// 2.0</span></div>
<div class="line"> </div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> y2 = x * 3;</div>
<div class="line">y2.backward();</div>
<div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;Accumulated: &quot;</span> &lt;&lt; x.grad() &lt;&lt; std::endl;  <span class="comment">// 5.0 (2.0 + 3.0)</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Clear gradients before next computation</span></div>
<div class="line">x.zero_grad();</div>
</div><!-- fragment --><p>Important**: Always call <code>zero_grad()</code> before computing new gradients if you don't want accumulation.</p>
<h2><a class="anchor" id="autotoc_md28"></a>
How gradients flow</h2>
<p>Understanding gradient flow helps debug neural networks:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> x(data, <span class="keyword">true</span>);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> a = x * 2;      <span class="comment">// da/dx = 2</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> b = lmp::exp(a); <span class="comment">// db/da = exp(a)</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> c = lmp::sum(b); <span class="comment">// dc/db = 1</span></div>
<div class="line"> </div>
<div class="line">c.backward();</div>
<div class="line"><span class="comment">// dc/dx = dc/db * db/da * da/dx = 1 * exp(2x) * 2 = 2 * exp(2x)</span></div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md29"></a>
Training loop structure</h3>
<div class="fragment"><div class="line"><span class="comment">// Neural network parameters</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> weights(weight_data, <span class="keyword">true</span>);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> bias(bias_data, <span class="keyword">true</span>);</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">int</span> epoch = 0; epoch &lt; num_epochs; ++epoch) {</div>
<div class="line">    <span class="keywordflow">for</span> (<span class="keyword">auto</span>&amp; batch : data_loader) {</div>
<div class="line">        <span class="comment">// Clear gradients from previous iteration</span></div>
<div class="line">        weights.zero_grad();</div>
<div class="line">        bias.zero_grad();</div>
<div class="line">        </div>
<div class="line">        <span class="comment">// Forward pass</span></div>
<div class="line">        <a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> output = lmp::matmul(batch.input, weights) + bias;</div>
<div class="line">        <a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> loss = compute_loss(output, batch.target);</div>
<div class="line">        </div>
<div class="line">        <span class="comment">// Backward pass</span></div>
<div class="line">        loss.backward();</div>
<div class="line">        </div>
<div class="line">        <span class="comment">// Update parameters</span></div>
<div class="line">        <span class="keywordtype">float</span> learning_rate = 0.01f;</div>
<div class="line">        weights = <a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a>(weights.data() - learning_rate * weights.grad(), <span class="keyword">true</span>);</div>
<div class="line">        bias = <a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a>(bias.data() - learning_rate * bias.grad(), <span class="keyword">true</span>);</div>
<div class="line">    }</div>
<div class="line">}</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md30"></a>
Understanding the computation graph</h2>
<p>When you call <code>backward()</code>, Lamp++ performs a topological sort to ensure gradients are computed in the right order:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> x(data, <span class="keyword">true</span>);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> y = x * x;</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> loss = lmp::sum(y);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// backward() visits nodes in reverse topological order</span></div>
<div class="line">loss.backward();</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md31"></a>
Working with gradients</h2>
<h3><a class="anchor" id="autotoc_md32"></a>
Checking and manipulating gradients</h3>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> var(data, <span class="keyword">true</span>);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Check gradient status</span></div>
<div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;Requires grad: &quot;</span> &lt;&lt; var.requires_grad() &lt;&lt; std::endl;</div>
<div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;Has grad_fn: &quot;</span> &lt;&lt; (var.grad_fn() != <span class="keyword">nullptr</span>) &lt;&lt; std::endl;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Manual gradient manipulation</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> custom_grad(grad_data, var.data().shape(), var.data().device());</div>
<div class="line">var.incr_grad(custom_grad);  <span class="comment">// Add to existing gradients</span></div>
<div class="line">var.zero_grad();             <span class="comment">// Clear gradients</span></div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md33"></a>
Debugging gradients</h3>
<div class="fragment"><div class="line"><span class="comment">// Check gradient magnitude</span></div>
<div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;Gradient magnitude: &quot;</span> &lt;&lt; lmp::sum(param.grad() * param.grad()) &lt;&lt; std::endl;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Check for NaN gradients</span></div>
<div class="line"><span class="keyword">auto</span> grad_vector = param.grad().to_vector&lt;<span class="keywordtype">float</span>&gt;();</div>
<div class="line"><span class="keywordtype">bool</span> has_nan = std::any_of(grad_vector.begin(), grad_vector.end(), </div>
<div class="line">                          [](<span class="keywordtype">float</span> x) { return std::isnan(x); });</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md34"></a>
Performance considerations</h2>
<h3><a class="anchor" id="autotoc_md35"></a>
Memory usage</h3>
<ul>
<li>Each operation stores references to its inputs for the backward pass</li>
<li>Computation graphs can use significant memory for long sequences</li>
<li>Clear gradients with <code>zero_grad()</code> when you don't need them</li>
</ul>
<h3><a class="anchor" id="autotoc_md36"></a>
When to use tensors vs. variables</h3>
<p>Use Tensors for:**</p><ul>
<li>Data loading and preprocessing</li>
<li>Operations where you don't need gradients</li>
<li><p class="startli">Performance-critical code paths</p>
<p class="startli">Use Variables for:**</p>
</li>
<li>Model parameters</li>
<li>Any computation where you need gradients</li>
<li>Forward and backward passes in training</li>
</ul>
<h2><a class="anchor" id="autotoc_md37"></a>
Complete example</h2>
<p>For a complete neural network implementation using all these concepts, see the MNIST example in <code>examples/mnist.cpp</code>. It demonstrates:</p><ul>
<li><a class="el" href="classParameter.html">Parameter</a> initialization with <code>autograd::rand()</code></li>
<li>Forward pass with <code>matmul()</code>, activation functions, and loss computation</li>
<li>Backward pass and gradient-based parameter updates</li>
<li>Proper gradient clearing in training loops</li>
</ul>
<p>The key insight is that autograd turns the tedious job of computing derivatives into an automatic process, letting you focus on the interesting parts of machine learning: designing architectures and solving problems. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
