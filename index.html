<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>lamppp: Lamp++ Documentation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">lamppp
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Lamp++ Documentation </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Lamp++ is a C++ automatic differentiation engine and tensor library built for people who want to understand what's happening under the hood. If you've ever looked into Pytorch's codebase (complex macros, codegens scripts, like wtf?) and been frustrated by PyTorch's black-box approach, or if you wanted to learn how autograd actually works, Lamp++ is for you.</p>
<p>I built this library from scratch in about 4,000 lines of C++ because I believe in Richard Feynman's quote "what I cannot create, I do not understand". You can read the entire codebase in one afternoon, understand the entire codebase in two afternoons, and be ready to build/extend the codebase in the third (I hope).</p>
<h2><a class="anchor" id="autotoc_md10"></a>
What makes Lamp++ different?</h2>
<p>It's small and readable.** The entire autograd engine is ~1,500 lines. The tensor library is ~2,500 lines. You can actually understand how it works.</p>
<p>It's built for learning.** Want to see how backward propagation works? Look at <code>autograd/Variable.hpp</code>. Curious about CUDA kernels? Check out <code>src/tensor/cuda/</code>. Everything is there, nothing is hidden.</p>
<p>It gets out of your way.** No heavyweight framework overhead. No mysterious compilation steps. Just tensors, gradients, and the operations you need.</p>
<h2><a class="anchor" id="autotoc_md11"></a>
Getting started</h2>
<p>The fastest way to see what Lamp++ can do is to try this example:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;lamppp/lamppp.hpp&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">int</span> main() {</div>
<div class="line">    <span class="comment">// Create some tensors</span></div>
<div class="line">    <a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> data_a(std::vector&lt;float&gt;{2.0F, 4.0F}, {1, 2}, </div>
<div class="line">                       lmp::DeviceType::CUDA, lmp::DataType::Float32);</div>
<div class="line">    <a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> data_b(std::vector&lt;float&gt;{3.0F, 1.0F}, {1, 2}, </div>
<div class="line">                       lmp::DeviceType::CUDA, lmp::DataType::Float32);</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// Wrap them in Variables to track gradients</span></div>
<div class="line">    <a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> a(data_a, <span class="keyword">true</span>);</div>
<div class="line">    <a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> b(data_b, <span class="keyword">true</span>);</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// Do some math</span></div>
<div class="line">    <a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> c = a * b;</div>
<div class="line">    <a class="code hl_class" href="classlmp_1_1autograd_1_1Variable.html">lmp::Variable</a> loss = lmp::sum(lmp::sum(c, 1), 0);</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// Compute gradients</span></div>
<div class="line">    loss.backward();</div>
<div class="line"> </div>
<div class="line">    <span class="comment">// See what happened</span></div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Gradient of a: &quot;</span> &lt;&lt; a.grad() &lt;&lt; std::endl;</div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Gradient of b: &quot;</span> &lt;&lt; b.grad() &lt;&lt; std::endl;</div>
<div class="line">}</div>
<div class="ttc" id="aclasslmp_1_1autograd_1_1Variable_html"><div class="ttname"><a href="classlmp_1_1autograd_1_1Variable.html">lmp::autograd::Variable</a></div><div class="ttdef"><b>Definition</b> variable.hpp:37</div></div>
<div class="ttc" id="aclasslmp_1_1tensor_1_1Tensor_html"><div class="ttname"><a href="classlmp_1_1tensor_1_1Tensor.html">lmp::tensor::Tensor</a></div><div class="ttdoc">Main tensor object for Lamppp.</div><div class="ttdef"><b>Definition</b> tensor.hpp:29</div></div>
</div><!-- fragment --><p>To build this, you need CMake and optionally CUDA:</p>
<div class="fragment"><div class="line">git clone https://github.com/clay-arras/lamp.git</div>
<div class="line">cd lamp</div>
<div class="line">cmake -S . -B build -DLMP_ENABLE_CUDA=ON  # or OFF if you don&#39;t have CUDA</div>
<div class="line">cmake --build build</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md12"></a>
How it works</h2>
<p>Lamp++ builds computation graphs dynamically as you run your code. When you do <code>a * b</code>, it creates a multiplication node that remembers how to compute gradients. When you call <code>loss.backward()</code>, it walks backward through the graph computing derivatives using the chain rule. If you'd like to learn more, watch this video by Andej Kaparthy, he teaches everything you need to know about the autograd module in a very straightforward manner <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&">https://www.youtube.com/watch?v=VMj-3S1tku0&amp;</a></p>
<p>The tensor library stores everything as <code>void*</code> with type information, and during operations the pointers are casted back to their original types. CUDA operations have their own kernels in <code>src/tensor/cuda/</code>, while CPU operations live in <code>src/tensor/native/</code>.</p>
<p>If you want to add a new operation, inherit from <code>autograd::Function</code> and implement <code>forward</code> and <code>backward</code> methods. That's all there is to it.</p>
<h2><a class="anchor" id="autotoc_md13"></a>
What's in the docs</h2>
<ul>
<li><a class="el" href="getting_started.html">Getting Started</a> - Build your first neural network</li>
<li><a class="el" href="using_tensor.html">Working with Tensors</a> - Shapes, devices, and operations <br  />
</li>
<li><a class="el" href="using_autograd.html">Understanding Autograd</a> - How gradients actually work</li>
</ul>
<h2><a class="anchor" id="autotoc_md14"></a>
This probably isn't for you if...</h2>
<p>You want a production-ready framework with every layer type imaginable. That's PyTorch's job, and it does it well.</p>
<p>You need maximum performance for huge models. Lamp++ prioritizes clarity over cutting-edge optimization.</p>
<p>You don't care how automatic differentiation works. If you just want to train models, stick with the mainstream tools.</p>
<h2><a class="anchor" id="autotoc_md15"></a>
This might be perfect if...</h2>
<p>You're learning how neural networks actually work and want to peek inside the machinery.</p>
<p>You're prototyping new operations or optimization algorithms and need something you can easily modify.</p>
<p>You're building something specific where you need control over every detail.</p>
<p>You enjoy reading well-structured C++ code and want to understand how modern ML libraries work.</p>
<h2><a class="anchor" id="autotoc_md16"></a>
Contributing</h2>
<p>If you find bugs or want to add features, please jump in! The codebase is small enough that you can understand the whole thing, and we're happy to help you get oriented.</p>
<p>The most useful contributions right now are:</p><ul>
<li>New tensor operations</li>
<li>Better documentation and examples <br  />
</li>
<li>Performance improvements for existing operations</li>
<li>Support for additional data types</li>
</ul>
<h2><a class="anchor" id="autotoc_md17"></a>
License</h2>
<p>MIT License. Use it however you want. </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
