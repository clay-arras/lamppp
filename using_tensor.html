<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>lamppp: Working with Tensors</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">lamppp
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="index.html">Lamp++ Documentation</a></li>  </ul>
</div>
</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">Working with Tensors</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Tensors are the fundamental data structure in Lamp++. They're n-dimensional arrays that can live on CPU or GPU, store different data types, and support automatic broadcasting for operations (it's like what ATen is for Pytorch, or if NumPy had CUDA support). This guide covers everything you need to know to work with them effectively.</p>
<h2><a class="anchor" id="autotoc_md38"></a>
Creating tensors</h2>
<h3><a class="anchor" id="autotoc_md39"></a>
From vectors</h3>
<p>The most common way to create a tensor is from a C++ vector:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;lamppp/lamppp.hpp&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Simple 1D tensor</span></div>
<div class="line">std::vector&lt;float&gt; data = {1.0f, 2.0f, 3.0f, 4.0f};</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> tensor(data, {4}, lmp::DeviceType::CPU, lmp::DataType::Float32);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// 2D tensor (matrix)</span></div>
<div class="line">std::vector&lt;int&gt; matrix_data = {1, 2, 3, 4, 5, 6};</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> matrix(matrix_data, {2, 3}, lmp::DeviceType::CPU, lmp::DataType::Int32);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// 3D tensor</span></div>
<div class="line">std::vector&lt;double&gt; cube_data(24, 1.0);  <span class="comment">// 24 elements, all 1.0</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> cube(cube_data, {2, 3, 4}, lmp::DeviceType::CPU, lmp::DataType::Float64);</div>
<div class="ttc" id="aclasslmp_1_1tensor_1_1Tensor_html"><div class="ttname"><a href="classlmp_1_1tensor_1_1Tensor.html">lmp::tensor::Tensor</a></div><div class="ttdoc">Main tensor object for Lamppp.</div><div class="ttdef"><b>Definition</b> tensor.hpp:29</div></div>
</div><!-- fragment --><p>The constructor takes four parameters:</p><ul>
<li><b>data</b>: A flat vector in row-major order</li>
<li><b>shape</b>: Dimensions as a vector (e.g., <code>{28, 28}</code> for a 28×28 image)</li>
<li><b>device</b>: Where to store the tensor (<code>CPU</code> or <code>CUDA</code>)</li>
<li><b>dtype</b>: Element type (defaults to <code>Float64</code>)</li>
</ul>
<h3><a class="anchor" id="autotoc_md40"></a>
Data types</h3>
<p>Lamp++ supports six data types:</p>
<div class="fragment"><div class="line"><span class="comment">// Integer types</span></div>
<div class="line">lmp::DataType::Bool     </div>
<div class="line">lmp::DataType::Int16    </div>
<div class="line">lmp::DataType::Int32    </div>
<div class="line">lmp::DataType::Int64    </div>
<div class="line"> </div>
<div class="line"><span class="comment">// Floating-point types</span></div>
<div class="line">lmp::DataType::Float32  </div>
<div class="line">lmp::DataType::Float64  </div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md41"></a>
Device placement</h3>
<p>Tensors can live on CPU or GPU:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> cpu_tensor(data, {2, 2}, lmp::DeviceType::CPU);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> gpu_tensor(data, {2, 2}, lmp::DeviceType::CUDA);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Move between devices</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> moved_tensor = cpu_tensor.<a class="code hl_function" href="classlmp_1_1tensor_1_1Tensor.html#aa7d6b7a69b19532e7e33720b1008c73f">to</a>(lmp::DeviceType::CUDA);</div>
<div class="ttc" id="aclasslmp_1_1tensor_1_1Tensor_html_aa7d6b7a69b19532e7e33720b1008c73f"><div class="ttname"><a href="classlmp_1_1tensor_1_1Tensor.html#aa7d6b7a69b19532e7e33720b1008c73f">lmp::tensor::Tensor::to</a></div><div class="ttdeci">Tensor to(DeviceType device) const</div><div class="ttdef"><b>Definition</b> tensor.cpp:38</div></div>
</div><!-- fragment --><p>Important**: The <code>.to()</code> method creates a <b>new tensor</b> with copied data, unlike PyTorch which returns a view.</p>
<h2><a class="anchor" id="autotoc_md42"></a>
Tensor properties</h2>
<p>Every tensor has several properties you can query:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> tensor(data, {2, 3, 4});</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Shape and size</span></div>
<div class="line"><span class="keyword">auto</span> shape = tensor.shape();           <span class="comment">// {2, 3, 4}</span></div>
<div class="line"><span class="keywordtype">size_t</span> total_elements = tensor.numel(); <span class="comment">// 24</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Type and device</span></div>
<div class="line">lmp::DataType dtype = tensor.type();     <span class="comment">// DataType::Float64</span></div>
<div class="line">lmp::DeviceType device = tensor.device(); <span class="comment">// DeviceType::CPU</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Raw data pointer (advanced usage)</span></div>
<div class="line"><span class="keywordtype">void</span>* raw_data = tensor.data();</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md43"></a>
Converting back to vectors</h3>
<p>To get data out of a tensor:</p>
<div class="fragment"><div class="line">std::vector&lt;float&gt; result = tensor.to_vector&lt;<span class="keywordtype">float</span>&gt;();</div>
</div><!-- fragment --><p>This works regardless of the tensor's original data type - it handles the conversion automatically.</p>
<h2><a class="anchor" id="autotoc_md44"></a>
Shape manipulation</h2>
<p>Tensors support several operations that change their shape without copying data:</p>
<h3><a class="anchor" id="autotoc_md45"></a>
Reshaping</h3>
<p>Reshaping is a fast operation that doesn't change the underlying data. Lamp++ does not support non-contiguous tensors.</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> original(data, {2, 6});    <span class="comment">// 2×6 matrix</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> reshaped = original.<a class="code hl_function" href="classlmp_1_1tensor_1_1Tensor.html#ab572f9bf0d7c8d5b0ce0e9e2cb79a9e3">reshape</a>({3, 4});  <span class="comment">// 3×4 matrix</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> flattened = original.<a class="code hl_function" href="classlmp_1_1tensor_1_1Tensor.html#ab572f9bf0d7c8d5b0ce0e9e2cb79a9e3">reshape</a>({12});   <span class="comment">// 1D vector</span></div>
<div class="ttc" id="aclasslmp_1_1tensor_1_1Tensor_html_ab572f9bf0d7c8d5b0ce0e9e2cb79a9e3"><div class="ttname"><a href="classlmp_1_1tensor_1_1Tensor.html#ab572f9bf0d7c8d5b0ce0e9e2cb79a9e3">lmp::tensor::Tensor::reshape</a></div><div class="ttdeci">Tensor reshape(std::vector&lt; size_t &gt; new_shape) const</div><div class="ttdef"><b>Definition</b> tensor.cpp:25</div></div>
</div><!-- fragment --><p>Note**: The total number of elements must remain the same.</p>
<h3><a class="anchor" id="autotoc_md46"></a>
Adding and removing dimensions</h3>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> tensor(data, {2, 3});</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Add a dimension</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> expanded = tensor.expand_dims(0);  <span class="comment">// Shape becomes {1, 2, 3}</span></div>
<div class="line">expanded = tensor.expand_dims(2);              <span class="comment">// Shape becomes {2, 3, 1}</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Remove a dimension of size 1</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> squeezed = expanded.squeeze(0);    <span class="comment">// Back to {2, 3}</span></div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md47"></a>
Transposition</h3>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> matrix(data, {3, 4});</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> transposed = lmp::transpose(matrix);  <span class="comment">// Shape: {4, 3}</span></div>
</div><!-- fragment --><p>Important**: Unlike PyTorch, <code>transpose()</code> returns a new tensor, not a view.</p>
<h2><a class="anchor" id="autotoc_md48"></a>
Element-wise operations</h2>
<h3><a class="anchor" id="autotoc_md49"></a>
Arithmetic operations</h3>
<p>All basic arithmetic operations work element-wise and support broadcasting:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> a(data_a, {2, 3});</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> b(data_b, {2, 3});</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Basic arithmetic</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> sum = a + b;      <span class="comment">// Element-wise addition</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> diff = a - b;     <span class="comment">// Element-wise subtraction  </span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> product = a * b;  <span class="comment">// Element-wise multiplication</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> quotient = a / b; <span class="comment">// Element-wise division</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> power = lmp::pow(a, b); <span class="comment">// Element-wise power</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// With scalars</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> scaled = a * 2.0f;     <span class="comment">// Multiply all elements by 2</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> shifted = a + 1.0f;    <span class="comment">// Add 1 to all elements</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> from_scalar = 3.0f * a; <span class="comment">// Scalar-tensor multiplication</span></div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md50"></a>
Mathematical functions</h3>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> input(data, {2, 3});</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Unary math functions</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> negated = -input;              <span class="comment">// or lmp::neg(input)</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> exponential = lmp::exp(input);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> logarithm = lmp::log(input);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> square_root = lmp::sqrt(input);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> absolute = lmp::abs(input);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Trigonometric functions</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> sine = lmp::sin(input);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> cosine = lmp::cos(input);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> tangent = lmp::tan(input);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Clamping (ReLU-like)</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> clamped = lmp::clamp(input, 0.0f, 1.0f);  <span class="comment">// Clamp between 0 and 1</span></div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md51"></a>
Comparison operations</h3>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> a(data_a, {2, 3});</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> b(data_b, {2, 3});</div>
<div class="line"> </div>
<div class="line"><span class="comment">// All return boolean tensors</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> equal = (a == b);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> not_equal = (a != b);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> greater = (a &gt; b);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> greater_equal = (a &gt;= b);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> less = (a &lt; b);</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> less_equal = (a &lt;= b);</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md52"></a>
Broadcasting</h2>
<p>Lamp++ follows NumPy broadcasting rules. When operating on tensors with different shapes, they're automatically aligned:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> matrix(data, {3, 4});           <span class="comment">// 3×4 matrix</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> vector(vector_data, {4});       <span class="comment">// 1D vector with 4 elements</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> scalar_tensor(scalar_data, {1}); <span class="comment">// Single element</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// These all work due to broadcasting</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> result1 = matrix + vector;       <span class="comment">// Vector broadcasts to {3, 4}</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> result2 = matrix * scalar_tensor; <span class="comment">// Scalar broadcasts to {3, 4}</span></div>
</div><!-- fragment --><p>Broadcasting rules**:</p><ol type="1">
<li>Align shapes from the rightmost dimension</li>
<li>Dimensions of size 1 are "stretched" to match</li>
<li>Missing dimensions are treated as size 1</li>
</ol>
<p>Examples of valid broadcasts:</p><ul>
<li><code>{3, 4}</code> + <code>{4}</code> → both become <code>{3, 4}</code></li>
<li><code>{2, 3, 4}</code> + <code>{1, 4}</code> → both become <code>{2, 3, 4}</code></li>
<li><code>{5, 1, 3}</code> + <code>{2, 3}</code> → both become <code>{5, 2, 3}</code></li>
</ul>
<h2><a class="anchor" id="autotoc_md53"></a>
Reduction operations</h2>
<p>Reductions compute aggregates along specified axes:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> tensor(data, {2, 3, 4});</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Reduce along axis 0 (first dimension)</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> sum_axis0 = lmp::sum(tensor, 0);     <span class="comment">// Shape: {1, 3, 4}</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> max_axis1 = lmp::max(tensor, 1);     <span class="comment">// Shape: {2, 1, 4}</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> min_axis2 = lmp::min(tensor, 2);     <span class="comment">// Shape: {2, 3, 1}</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> prod_axis0 = lmp::prod(tensor, 0);   <span class="comment">// Product along axis 0</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Remove singleton dimensions if desired</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> collapsed = sum_axis0.squeeze(0);    <span class="comment">// Shape: {3, 4}</span></div>
</div><!-- fragment --><p>Note**: All reduction operations keep dimensions by default (like <code>keepdims=True</code> in NumPy). Use <code>squeeze()</code> to remove singleton dimensions.</p>
<h2><a class="anchor" id="autotoc_md54"></a>
Matrix operations</h2>
<p>For linear algebra operations:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> a(data_a, {3, 4});</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> b(data_b, {4, 5});</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Matrix multiplication</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> result = lmp::matmul(a, b);  <span class="comment">// Shape: {3, 5}</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Works with batched matrices too</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> batch_a(batch_data_a, {2, 3, 4}); <span class="comment">// Batch of 2 matrices</span></div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> batch_b(batch_data_b, {2, 4, 5});</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> batch_result = lmp::matmul(batch_a, batch_b); <span class="comment">// Shape: {2, 3, 5}</span></div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md55"></a>
Memory management and copying</h2>
<h3><a class="anchor" id="autotoc_md56"></a>
Views vs. copies</h3>
<p>Some operations return views (sharing memory):</p><ul>
<li><code>reshape()</code>, <code>squeeze()</code>, <code>expand_dims()</code> - return views</li>
<li>Most mathematical operations - return new tensors</li>
<li><code>to()</code> - always returns a new tensor</li>
</ul>
<h3><a class="anchor" id="autotoc_md57"></a>
Explicit copying</h3>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> original(data, {2, 3});</div>
<div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> target(target_data, {2, 3});</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Copy data from another tensor</span></div>
<div class="line">target.<a class="code hl_function" href="classlmp_1_1tensor_1_1Tensor.html#aca131b4b0fb7585bc9a170e9e31afe62">copy</a>(original);  <span class="comment">// Modifies target in-place</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Fill with a constant value</span></div>
<div class="line">target.fill(3.14f);     <span class="comment">// All elements become 3.14</span></div>
<div class="ttc" id="aclasslmp_1_1tensor_1_1Tensor_html_aca131b4b0fb7585bc9a170e9e31afe62"><div class="ttname"><a href="classlmp_1_1tensor_1_1Tensor.html#aca131b4b0fb7585bc9a170e9e31afe62">lmp::tensor::Tensor::copy</a></div><div class="ttdeci">void copy(const Tensor &amp;other)</div><div class="ttdef"><b>Definition</b> tensor.cpp:46</div></div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md58"></a>
Working with specific elements</h2>
<h3><a class="anchor" id="autotoc_md59"></a>
Indexing</h3>
<div class="fragment"><div class="line"><a class="code hl_class" href="classlmp_1_1tensor_1_1Tensor.html">lmp::Tensor</a> tensor(data, {2, 3});</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Access individual elements</span></div>
<div class="line">lmp::Scalar element = tensor.index({1, 2});  <span class="comment">// Element at row 1, column 2</span></div>
<div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;Value: &quot;</span> &lt;&lt; element &lt;&lt; std::endl;</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md60"></a>
Performance considerations</h2>
<h3><a class="anchor" id="autotoc_md61"></a>
Device-specific optimizations</h3>
<ul>
<li><b>CPU operations</b> use OpenMP for parallelization</li>
<li><b>CUDA operations</b> have custom kernels in <code>src/tensor/cuda/</code></li>
<li><b>Mixed device operations</b> require explicit transfers</li>
</ul>
<h3><a class="anchor" id="autotoc_md62"></a>
Memory layout</h3>
<p>Tensors use row-major (C-style) memory layout:</p><ul>
<li><code>{2, 3}</code> tensor: <code>[row0_col0, row0_col1, row0_col2, row1_col0, row1_col1, row1_col2]</code></li>
<li>Strides are calculated automatically for efficient memory access</li>
</ul>
<h3><a class="anchor" id="autotoc_md63"></a>
Type promotion</h3>
<p>When operating on tensors with different types, Lamp++ promotes to the "higher" type:</p><ul>
<li><code>Bool</code> &lt; <code>Int16</code> &lt; <code>Int32</code> &lt; <code>Int64</code> &lt; <code>Float32</code> &lt; <code>Float64</code></li>
<li>Example: <code>Int32</code> + <code>Float32</code> → <code>Float32</code></li>
</ul>
<h2><a class="anchor" id="autotoc_md64"></a>
Next steps</h2>
<p>Now that you understand tensors, you're ready to learn about automatic differentiation in the <a class="el" href="using_autograd.html">Understanding Autograd</a> guide. The autograd system builds on these tensor operations to compute gradients automatically. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
